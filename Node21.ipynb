{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3a9d916fb0849f6",
   "metadata": {},
   "source": [
    "# Projecte Node21: Classificació i Detecció de Nòduls Pulmonars en Radiografies de Pit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f0f5ae-8443-4d9e-a8e6-a3881d8d775f",
   "metadata": {},
   "source": [
    "## Introducció\n",
    "En aquest projecte abordarem la detecció de nòduls pulmonars en radiografies de tòrax (CXR) seguint l'enunciat del repte **Node21** ([enllaç al repte](https://node21.grand-challenge.org/)). El repte Node21 proporciona un conjunt de dades públic de radiografies frontals de pit amb nòduls pulmonars anotats mitjançant **caixes delimitadores** (bounding boxes). En total consta d'aproximadament **4.882 radiografies**, de les quals **1.134** contenen almenys un nòdul i **3.748** imatges no en contenen, actuant com a casos negatius.  \n",
    "\n",
    "L'objectiu consisteix en aplicar tècniques d'aprenentatge automàtic al problema proposat. S'han definit un **conjunt de tasques seqüencials** per tal de facilitar-ne el desenvolupament:\n",
    "\n",
    "1. **Classificació**: Desenvolupar un sistema de classificació binària capaç de determinar si una radiografia conté nòduls pulmonars. Aquesta tasca es resoldrà mitjançant l'avaluació de **quatre models diferents**, alguns d'entrenats des de zero i d'altres aprofitant tècniques de *transfer learning*.\n",
    "2. **Detecció**: Implementar models de detecció per localitzar els nòduls en la imatge, identificant-ne la posició amb caixes delimitadores. Per aquesta tasca es faran servir **dues arquitectures diferents** basades en *deep learning*.\n",
    "3. **Innovació**: S’obre la possibilitat d’aplicar tècniques avançades o enfocaments propis que ampliïn o millorin les metodologies aplicades.\n",
    "\n",
    "Per abordar aquestes tasques, utilitzarem un únic *notebook* Jupyter que integrarà tot el codi i explicacions necessàries. S'empraran diverses tècniques i models d'**aprenentatge automàtic** i **aprenentatge profund** vists a classe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffd2b86-cb8f-4cbc-a409-4dc83576452c",
   "metadata": {},
   "source": [
    "## Preparació de l'entorn i dependències\n",
    "Abans de començar, assegurem-nos de tenir instal·lades totes les **dependències** necessàries. El projecte requerirà les biblioteqües següents:\n",
    "\n",
    "- **NumPy** - per a manipulació de dades numèriques i de taules (anotacions).\n",
    "- **PyTorch** - per construir i entrenar els models de CNN i Transfer Learning.\n",
    "- **Matplotlib** - per a la visualizació de gràfiques i imatges.\n",
    "\n",
    "Podem instal·lar els paquets que falten directament des del notebool. Per exemple:\n",
    "\n",
    "`pip install torch torchvision scikit-learn matplotlib numpy`\n",
    "\n",
    "També assegurarem una estructura de carpetes correcta en el directori de treball actual:\n",
    "\n",
    "- `data/` - Contindrà les dades del NODE21 (imatges i anotacions).\n",
    "- `utils/` - Codi de suport reutilitzable, com ara funcions d’augmentació, classes Dataset, funcions per dibuixar caixes, etc.\n",
    "- `outputs/` - Elements de sortida.\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097e127b-aeec-430c-8e59-21814f6a46f5",
   "metadata": {},
   "source": [
    "## Obtenció i preparació de les dades Node21\n",
    "\n",
    "Per dur a terme les tasques de classificació i detecció de nòduls pulmonars, utilitzarem les dades proporcionades pel repte NODE21, descarregades des del repositori oficial de Zenodo. Aquest conjunt inclou radiografies de tòrax simulades amb nòduls inserits artificialment i anotacions detallades sobre la seva posició.\n",
    "\n",
    "A causa de la mida del conjunt complet (~35 GB), no resulta eficient entrenar directament amb totes les imatges en un entorn personal. Per això, optarem per treballar amb un subconjunt configurable del total d’imatges, mantinguent una distribució equilibrada entre imatges positives (amb nòduls) i negatives (sense nòduls), que és fonamental per garantir una bona generalització del model.\n",
    "\n",
    "Les imatges es troben dins la carpeta:\n",
    "\n",
    "`data/cxr_images/processed_data/images/` \n",
    "\n",
    "i tenen format **.mha**. Les anotacions associades es troben al fitxer \n",
    "\n",
    "`data/cxr_images/processed_data/simulated_metadata.csv`\n",
    "\n",
    "Aquest fitxer indica, per a cada nòdul simulat, a quina imatge apareix i amb quines coordenades **(x, y, width, height)**. Les imatges que no apareixen en aquest fitxer es consideren **negatives** (sense nòduls). Aquest enfocament ens permet construir tant:\n",
    "\n",
    "- **Etiquetes binàries** per a classificació: 1 si hi ha algun nòdul, 0 si no.\n",
    "- **Bounding boxes per detecció**: una o més caixes per imatge, o cap."
   ]
  },
  {
   "cell_type": "code",
   "id": "7044ce04-7f22-413a-b54a-8796dffedf9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T12:41:52.461121Z",
     "start_time": "2025-12-22T12:41:47.928207Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ruta a les imatges i anotacions\n",
    "IMG_DIR = \"data/cxr_images/proccessed_data/images\"\n",
    "ANNOTATION_FILE = \"./data/cxr_images/proccessed_data/simulated_metadata.csv\"\n",
    "\n",
    "# Nombre total d'imatges a utilitzar (ajustable)\n",
    "N_IMAGES = 500  # Exemple: 500 imatges en total (positives + negatives)\n",
    "VAL_RATIO = 0.25  # 25% per a validació\n",
    "\n",
    "# Mida d'entrada desitjada per al model (per exemple, 512x512)\n",
    "IMG_SIZE = 512"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "1a3e303e-24cf-438c-a771-60d86d99c40a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T12:41:53.473326Z",
     "start_time": "2025-12-22T12:41:52.475456Z"
    }
   },
   "source": [
    "# Carregam el CSV d'anotacions\n",
    "df = pd.read_csv(ANNOTATION_FILE)\n",
    "\n",
    "# Construïm diccionari de caixes per imatge\n",
    "annotations_dict = {}\n",
    "for _, row in df.iterrows():\n",
    "    img_id = row['img_name'].replace('.mha', '')  # treim extensió\n",
    "    box = [row['x'], row['y'], row['width'], row['height']]\n",
    "    annotations_dict.setdefault(img_id, []).append(box)\n",
    "\n",
    "# Construïm etiquetes binàries per classificació\n",
    "labels_dict = {img_id: 1 for img_id in annotations_dict}  # imatges amb nòdul\n",
    "\n",
    "# Llista completa d’imatges disponibles\n",
    "all_imgs = [f.replace('.mha', '') for f in os.listdir(IMG_DIR) if f.endswith('.mha')]\n",
    "\n",
    "# Afegim les negatives (sense anotació)\n",
    "for img_id in all_imgs:\n",
    "    if img_id not in annotations_dict:\n",
    "        annotations_dict[img_id] = []\n",
    "        labels_dict[img_id] = 0"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T12:41:55.027253Z",
     "start_time": "2025-12-22T12:41:54.990948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extracció estratificada d’un subconjunt manejable\n",
    "img_ids = list(labels_dict.keys())\n",
    "img_labels = [labels_dict[i] for i in img_ids]\n",
    "\n",
    "# Escollim N_IMAGES aleatòriament amb la mateixa proporció de classes\n",
    "_, subsampled_ids, _, _ = train_test_split(\n",
    "    img_ids, img_labels, test_size=N_IMAGES, stratify=img_labels, random_state=42)\n",
    "\n",
    "# Partim en entrenament i validació (mantenint estratificació)\n",
    "labels_subsampled = [labels_dict[i] for i in subsampled_ids]\n",
    "X_train, X_val = train_test_split(\n",
    "    subsampled_ids, test_size=VAL_RATIO, stratify=labels_subsampled, random_state=42)\n",
    "\n",
    "print(f\"Imatges seleccionades: {len(subsampled_ids)}\")\n",
    "print(f\"Entrenament: {len(X_train)} — Validació: {len(X_val)}\")"
   ],
   "id": "f6db34a4bdf5984",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imatges seleccionades: 500\n",
      "Entrenament: 375 — Validació: 125\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T13:13:49.657776Z",
     "start_time": "2025-12-22T13:13:49.623453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import SimpleITK as sitk\n",
    "\n",
    "img_p = sitk.ReadImage(\"data/cxr_images/proccessed_data/images/c0875.mha\")\n",
    "arr_p = sitk.GetArrayFromImage(img_p)\n",
    "\n",
    "print(f\"Forma:  {arr_p.shape}\")\n",
    "print(f\"Mínim: {arr_p.min()}\")\n",
    "print(f\"Màxim: {arr_p.max()}\")"
   ],
   "id": "bcc4a56861af1a4a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma:  (1024, 1024)\n",
      "Mínim: 0\n",
      "Màxim: 4095\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "La imatge és **2D** (1024, 1024). El rang és fix i conegut **[0,4095]** (representació de 12 bits)",
   "id": "ce0a48eb31b1d139"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-22T13:24:31.261266Z",
     "start_time": "2025-12-22T13:24:11.413928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "CONVERTIR .MHA A .PNG\n",
    "\n",
    "import SimpleITK as sitk\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Carpeta de sortida\n",
    "OUTPUT_DIR = f\"./data/preprocessed_{IMG_SIZE}\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def convert_and_resize(img_id, output_path):\n",
    "    mha_path = os.path.join(IMG_DIR, f\"{img_id}.mha\")\n",
    "    img = sitk.ReadImage(mha_path)\n",
    "    arr = sitk.GetArrayFromImage(img)\n",
    "          \n",
    "    # Normalització per percentils\n",
    "    p1, p99 = np.percentile(arr, (1, 99))\n",
    "    arr_clipped = np.clip(arr, p1, p99)\n",
    "    norm = ((arr_clipped - p1) / (p99 - p1)) * 255.0\n",
    "    norm = norm.astype(np.uint8)\n",
    "    \n",
    "    resized = cv2.resize(norm, (IMG_SIZE, IMG_SIZE))\n",
    "    cv2.imwrite(os.path.join(output_path, f\"{img_id}.png\"), resized)\n",
    "\n",
    "# Convertim les imatges seleccionades\n",
    "for img_id in X_train + X_val:\n",
    "    convert_and_resize(img_id, OUTPUT_DIR)\n",
    "\"\"\"\n"
   ],
   "id": "2c1b584874294515",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CXRClassificationDataSet(Dataset):\n",
    "    def __init__(self, img_ids, labels_dict, img_dir, img_size, transform=None):\n",
    "        self.img_ids = img_ids\n",
    "        self.labels_dict = labels_dict\n",
    "        self.img_dir = img_dir\n",
    "        self.img_size = img_size\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        label = self.labels_dict[img_id]\n",
    "        \n",
    "        # Ruta de la imatge .mha\n",
    "        path = os.path.join(self.img_dir, f\"{img_id}.mha\")\n",
    "        img = sitk.ReadImage(path)\n",
    "        arr = sitk.GetArrayFromImage(img)\n",
    "        \n",
    "        # Normalització per percentils (millora el contrast)\n",
    "        p1, p99 = np.percentile(arr, (1, 99))\n",
    "        arr = np.clip(arr, p1, p99)\n",
    "        arr = ((arr - p1) / (p99 - p1)) * 255.0\n",
    "        arr = arr.astype(np.uint8)\n",
    "        \n",
    "        # Redimensionar a img_size x img_size\n",
    "        arr_resized = cv2.resize(arr, (self.img_size, self.img_size))\n",
    "\n",
    "        # Convertir a tensor (1 canal, H, W) i normalitzar a [0,1]\n",
    "        tensor = torch.from_numpy(arr_resized).float().unsqueeze(0) / 255.0\n",
    "\n",
    "        if self.transform:\n",
    "            tensor = self.transform(tensor)\n",
    "\n",
    "        return tensor, torch.tensor(label, dtype=torch.long)"
   ],
   "id": "f2a71d807c631797"
  },
  {
   "cell_type": "markdown",
   "id": "c23dc184-94b0-4e87-91fd-d4e6d85e0cc6",
   "metadata": {},
   "source": [
    "## Classificació"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df51613-3869-4c9b-a682-e4eef0318cc8",
   "metadata": {},
   "source": [
    "### Model 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede996c7-6ab5-4419-98aa-51fa4d1b0ed6",
   "metadata": {},
   "source": [
    "### Model 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e56261a-1b34-4252-9303-254ad9a89750",
   "metadata": {},
   "source": [
    "### Model 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7427493-8f2b-4d11-822b-d59b1b13c7bd",
   "metadata": {},
   "source": [
    "### Model 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad56e655-82ec-4eea-a5f0-69ce2816fd9f",
   "metadata": {},
   "source": [
    "## Detecció"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0de1dd5-d896-4c18-a641-b8518e646dc2",
   "metadata": {},
   "source": [
    "## Innovació"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228bd04f-0eee-44a7-a257-16d3947ff789",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
